[model]
backend = "onnx"
device = "cpu"
model_path = "models/mnist.onnx"
gpu_ids = []

input_names = ["input"]
input_shapes = [[1, 1, 28, 28]]

output_names = ["output"]
output_shapes = [[1, 10]]

[queue]
max_batch = 1
max_wait_ms = 10

[redis]
url = "redis://127.0.0.1/"
out_prefix = "inference:out"

[pipeline]
pre = [
    "normalize(mean=0.5,std=0.5)",
    "resize(224,224)"
]
post = [
    "softmax",
    "argmax"
]